aws:
  # Credentials can be hardcoded or set in environment variables
  access_key_id: AKIAINTCJU27MPRPEQ4A
  secret_access_key: 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
  s3:
    region: eu-west-1
    buckets:
      assets: s3://snowplow-hosted-assets # DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket
      jsonpath_assets: s3://udmd-p-jsonpathfiles/  # If youmas, a your own JSON Path files in your own bucket here
      log: s3://udmd-p-etl/logs/
      raw:
        in:                  # Multiple in buckets are permitted
          - s3://udmd-global-p-raw-logs/  # e.g. s3://my-archive-bucket/ra
        processing: s3://udmd-p-etl/processing/
        archive: s3://udmd-p-archive/raw/    # e.g. s3://my-archive-bucket/raw
      enriched:
        good: s3://udmd-p-enriched/enrich/good/       # e.g. s3://my-out-bucket/enriched/good
        bad: s3://udmd-p-enriched/enrich/bad/      # e.g. s3://my-out-bucket/enriched/bad
        errors: s3://udmd-p-enriched/enrich/errors/     # Leave blank unless :continue_on_unexpected_error: set to true below
        archive: s3://udmd-p-archive/enrich/good/    # Where to archive enriched events to, e.g. s3://my-out-bucket/enriched/archive
      shredded:
        good: s3://udmd-p-enriched/shredded/good/      # e.g. s3://my-out-bucket/shredded/good
        bad: s3://udmd-p-enriched/shredded/bad/        # e.g. s3://my-out-bucket/shredded/bad
        errors: s3://udmd-p-enriched/shredded/errors/    # Leave blank unless :continue_on_unexpected_error: set to true below
        archive: s3://udmd-p-archive/shredded/good/    # Not required for Postgres currently
  emr:
    ami_version: 3.7.0      # Don't change this
    region: eu-west-1       # Always set this
    jobflow_role: EMR_EC2_DefaultRole # Created using $ aws emr create-default-roles
    service_role: EMR_DefaultRole     # Created using $ aws emr create-default-roles
    ec2_subnet_id: subnet-d8cf0681 # Set this if running in VPC. Leave blank otherwise
    ec2_key_name: udmd-p-eu-collector-key
    bootstrap: []           # Set this to specify custom boostrap actions. Leave empty otherwise
    software:
      hbase:                # To launch on cluster, provide version, "0.92.0", keep quotes
      lingual:              # To launch on cluster, provide version, "1.1", keep quotes
    # Adjust your Hadoop cluster below
    jobflow:
      master_instance_type: m1.medium
      core_instance_count: 3
      core_instance_type: m3.xlarge
      task_instance_count: 0 # Increase to use spot instances
      task_instance_type: m1.medium
      task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances
    bootstrap_failure_tries: 3 # Number of times to attempt the job in the event of bootstrap failures
collectors:
  format: clj-tomcat # Or 'clj-toClojure Collector, or 'thrift' for Thrift records, or 'tsv/com.amazon.aws.cloudfront/wd_access_log' for Cloudfront access logs
enrich:
  job_name: UDMD PROD # Give your job a name
  versions:
    hadoop_enrich: 1.5.0 # Version of the Hadoop Enrichment process
    hadoop_shred: 0.6.0 # Version of the Hadoop Shredding process
    hadoop_elasticsearch: 0.1.0 # Version of the Hadoop to Elasticsearch copying process
  continue_on_unexpected_error: true # Set to 'true' (and set :out_errors: above) if you don't want any exceptions thrown from ETL
  output_compression: NONE # Compression only supported with Redshift, set to NONE if you have Postgres targets. Allowed formats: NONE, GZIP
storage:
  download:
    folder: # Postgres-only config option. Where to store the downloaded files. Leave blank for Redshift
  targets:
    - name: "My Redshift database"
      type: redshift
      host: iedup-udmd-redshift.crmnvoqoidiu.eu-west-1.redshift.amazonaws.com  # The endpoint as shown in the Redshift console
      database: udmddb  # Name of database
      port: 5439 # Default Redshift port
      ssl_mode: disable # One of disable (default), require, verify-ca or verify-full
      table: landing.events
      username: udmddbadmin
      password: Fyf5Kuh8%wtSfd4Q
      maxerror: 1000 # Stop loading on first error, or increase to permit more load errors
      comprows: 200000 # Default for a 1 XL node cluster. Not used unless --include compupdate specified
    - name: "My Elasticsearch database"
      type: elasticsearch
      host: search-udmdanalysis-ehffztseumt7hppnm22spmqsby.eu-west-1.es.amazonaws.com # The Elasticsearch endpoint
      database: udmdanalysis # Name of index
      port: 80 # Default Elasticsearch port - change to 80 if using Amazon Elasticsearch Service
      sources:  # Leave blank sticsearch, or explicitly provide an array of bad row buckets like ["s3://my-enriched-bucket/bad/run=2015-10-06-15-25-53"]
      ssl_mode: # Not required for Elasticsearch
      table: bad_rows # Name of type
      username: # Not required for Elasticsearch
      password: # Not required for Elasticsearch
      es_nodes_wan_only: true # Set to true if using Amazon Elasticsearch Service
      maxerror: # Not required for Elasticsearch
      comprows: # Not required for Elasticsearch
monitoring:
  tags: {} # Name-value pairs describing this job
  logging:
    level: DEBUG # You can optionally switch to INFO for production
snowplow:
  method: get
  app_id: udmdid # e.g. snowplow
  collector:  # e.g. d3rkrsqld9gmqf.cloudfront.net
